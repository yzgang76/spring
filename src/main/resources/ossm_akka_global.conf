include classpath("host_and_port")

//It' not recommend to modify the follow configurations

akka {

  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
    loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "ERROR"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "ERROR"

  # Filter of log events that is used by the LoggingAdapter before
  # publishing log events to the eventStream.
  //  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  actor {
    provider = local
    allow-java-serialization = on
    warn-about-java-serializer-usage = false
  }
  serialization-bindings {
    "java.util.HashMap" = akka-misc
    "akka.cluster.metrics.ClusterMetricsMessage" = akka-cluster-metrics
    "akka.cluster.metrics.AdaptiveLoadBalancingPool" = akka-cluster-metrics
    "akka.cluster.metrics.MixMetricsSelector" = akka-cluster-metrics
    "akka.cluster.metrics.CpuMetricsSelector$" = akka-cluster-metrics
    "akka.cluster.metrics.HeapMetricsSelector$" = akka-cluster-metrics
    "akka.cluster.metrics.SystemLoadAverageMetricsSelector$" = akka-cluster-metrics
  }
  serialization-identifiers {
    "akka.cluster.metrics.protobuf.MessageSerializer" = 10
  }

  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    netty.tcp {
      hostname = ${HOST_LOCAL.HOST}
    }
  }
  remote.artery {
    canonical {
      hostname = ${HOST_LOCAL.HOST}
    }
  }
  cluster {
    seed-nodes = [
      ${Cluster_Manage_Node1}, ${?Cluster_Manage_Node2}
    ]
    jmx.multi-mbeans-in-same-jvm = off
    metrics {
      collector {
        enabled = on
        provider = ""
        fallback = true
        sample-interval = 30s
        gossip-interval = 30s
        moving-average-half-life = 60s
      }
    }
  }
//  extensions = ["akka.cluster.metrics.ClusterMetricsExtension"]
}
ossm.monitor {
  topics {
    metric = OSSM_BE_METRIC
    collectCMD = OSSM_BE_COLLECT_CMD
    service = OSSM_BE_SERVICE
  }
  cache {
    max-record-number = 1000
  }
}
//kafka {
//  active = ${KAFKA_IS_ACTIVE}
//  bootstrap-server = ${KAFKA_HOST}
//  port = ${HOST_LOCAL.PORTS.KAFKA}
//}
modules {  //defined spciafic configurations of each modules, that are not exposed to users.
  clusterManager {
    akka = ${akka} {
      remote {
        netty.tcp {
          port = ${HOST_LOCAL.PORTS.AKKA_CLUSTER_MANAGER}
        }
      }
      remote.artery {
        canonical {
          port = ${HOST_LOCAL.PORTS.AKKA_CLUSTER_MANAGER}
        }
      }

      management.http {
        hostname = ${HOST_LOCAL.HOST}
        port = ${HOST_LOCAL.PORTS.HTTP_CLUSTER_MANAGER}
        route-providers-read-only = false
      }
      kafka.consumer {
        # Tuning property of scheduled polls.
        # Controls the interval from one scheduled poll to the next.
        poll-interval = 50ms

        # Tuning property of the `KafkaConsumer.poll` parameter.
        # Note that non-zero value means that the thread that
        # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
        poll-timeout = 50ms

        # The stage will delay stopping the internal actor to allow processing of
        # messages already in the stream (required for successful committing).
        # Prefer use of `DrainingControl` over a large stop-timeout.
        stop-timeout = 30s

        # Duration to wait for `KafkaConsumer.close` to finish.
        close-timeout = 20s

        # If offset commit requests are not completed within this timeout
        # the returned Future is completed `CommitTimeoutException`.
        # The `Transactional.source` waits this ammount of time for the producer to mark messages as not
        # being in flight anymore as well as waiting for messages to drain, when rebalance is triggered.
        commit-timeout = 15s

        # If commits take longer than this time a warning is logged
        commit-time-warning = 1s

        # Not used anymore (since 1.0-RC1)
        # wakeup-timeout = 3s

        # Not used anymore (since 1.0-RC1)
        # max-wakeups = 10

        # If set to a finite duration, the consumer will re-send the last committed offsets periodically
        # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
        commit-refresh-interval = infinite

        # Not used anymore (since 1.0-RC1)
        # wakeup-debug = true

        # Fully qualified config path which holds the dispatcher configuration
        # to be used by the KafkaConsumerActor. Some blocking may occur.
        use-dispatcher = "akka.kafka.default-dispatcher"

        # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
        # can be defined in this configuration section.
        kafka-clients {
          # Disable auto-commit by default
          enable.auto.commit = false
        }

        # Time to wait for pending requests when a partition is closed
        wait-close-partition = 500ms

        # Limits the query to Kafka for a topic's position
        position-timeout = 5s

        # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
        # call to Kafka's API
        offset-for-times-timeout = 5s

        # Timeout for akka.kafka.Metadata requests
        # This value is used instead of Kafka's default from `default.api.timeout.ms`
        # which is 1 minute.
        metadata-request-timeout = 5s

        # Interval for checking that transaction was completed before closing the consumer.
        # Used in the transactional flow for exactly-once-semantics processing.
        eos-draining-check-interval = 30ms
      }
    }


  }
  //--------------------------------------------------------------------------------
  umbadapter {
    akka = ${akka}{
      remote {
        netty.tcp {
          port = ${HOST_LOCAL.PORTS.AKKA_UMBADAPTER}
        }
      }
      remote.artery {
        canonical {
          port = ${HOST_LOCAL.PORTS.AKKA_UMBADAPTER}
        }
      }
    }
    adapter {
      type = "umb_temip"
      clusteredCM = ${?HA.cmHA}
    }
  }
  //--------------------------------------------------------------------------------
  cmglobal {
    akka = ${akka}{
      remote {
        netty.tcp {
          port = ${HOST_LOCAL.PORTS.AKKA_CM}
        }
      }
      remote.artery {
        canonical {
          port = ${HOST_LOCAL.PORTS.AKKA_CM}
        }
      }
    }
  }
  //--------------------------------------------------------------------------------
  presenter {
    akka = ${akka}{
      loggers = ["akka.event.Logging$DefaultLogger"]
      remote {
        netty.tcp {
          port = ${HOST_LOCAL.PORTS.AKKA_PRESENTER}
        }
      }
      remote.artery {
        canonical {
          port = ${HOST_LOCAL.PORTS.AKKA_PRESENTER}
        }
      }

      quartz {
        schedules {
          forwardResult {
            description = "A cron job that fires off for check whether should sync counter to db"
            expression = "0 0 * * * ?"er
          }
        }
      }
    }

    configurations {
      rest_service_host = ${modules.clusterManager.akka.management.http.hostname}
      rest_service_port = ${modules.clusterManager.akka.management.http.port}
    }
  }
  //--------------------------------------------------------------------------------
  receiver {
    akka = ${akka}{
      remote {
        netty.tcp {
          port = ${HOST_LOCAL.PORTS.AKKA_RECEIVER}
          send-buffer-size = 2048000b
          receive-buffer-size = 1024000b
        }
        transport-failure-detector {
          implementation-class = "akka.remote.DeadlineFailureDetector"
          heartbeat-interval = 5 s
          acceptable-heartbeat-pause = 240 s
        }
        backoff-interval = 1s
        retry-gate-closed-for = 60 s
        system-message-buffer-size = 200000
        system-message-ack-piggyback-timeout = 1 s
      }
      remote.artery {
        canonical {
          port = ${HOST_LOCAL.PORTS.AKKA_RECEIVER}
        }
      }
    }
  }
  //--------------------------------------------------------------------------------
  receiverGlobal {
    startServer = true
    ServerParam = -tcpAllowOthers
    ServerPort = ${HOST_LOCAL.PORTS.H2_RECEIVER_DB}
    maxDBWorkerPerConsumer = 1
    remoteDataHandler {
      active = false
      path = "akka.tcp://RemoteDataHandler@"${HOST_LOCAL.HOST}":"${HOST_LOCAL.PORTS.AKKA_RECEIVER}"/user/dataHandler"
    }
    localDataHandler = "akka.tcp://ReceiverService@"${HOST_LOCAL.HOST}":"${HOST_LOCAL.PORTS.AKKA_RECEIVER}"/user/dataHandler"
  }
  //--------------------------------------------------------------------------------
  dcGlobal {
    akka = ${akka}{
      actor {
        deployment {
          /dataController {
            dispatcher = "blocking-io-dispatcher"
          }
          /statistics {
            dispatcher = "blocking-io-dispatcher"
          }
          /dbActor {
            dispatcher = "my-dispatcher"
          }
        }
      }
      remote {
        netty.tcp {
          port = ${HOST_LOCAL.PORTS.AKKA_DC_ADAPTER}
          send-buffer-size = 1024000b
          receive-buffer-size = 2048000b
        }
        enabled-transports = ["akka.remote.netty.tcp"]
        transport-failure-detector {
          implementation-class = "akka.remote.DeadlineFailureDetector"
          heartbeat-interval = 5 s
          acceptable-heartbeat-pause = 240 s
        }
        watch-failure-detector {
          implementation-class = "akka.remote.PhiAccrualFailureDetector"
          heartbeat-interval = 3 s
          threshold = 15.0
          max-sample-size = 200
          min-std-deviation = 300 ms
          acceptable-heartbeat-pause = 60 s
          unreachable-nodes-reaper-interval = 1s
          expected-response-after = 3 s
        }
        backoff-interval = 1s
        retry-gate-closed-for = 60 s
        system-message-buffer-size = 200000
        system-message-ack-piggyback-timeout = 1 s
      }
      remote.artery {
        canonical {
          port = ${HOST_LOCAL.PORTS.AKKA_DC_ADAPTER}
        }
      }
      quartz {
        schedules {
          SyncDb {
            description = "A cron job that fires off for check whether should sync counter to db"
            expression = "*/5 * * ? * *"
          }
          SyncUser {
            description = "A cron job that fires off for check whether should sync counter from user to cache"
            expression = "*/5 * * ? * *"
          }
          SyncRt {
            description = "A cron job that fires off for check whether should sync counter from time relative filters"
            expression = "*/10 * * ? * *"
          }
        }
      }
    }
    blocking-io-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 64
      }
      throughput = 1
    }
    my-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 1
        parallelism-factor = 3.0
        parallelism-max = 16
      }
      throughput = 100
    }
    dc {
      dbActorNumber = 1
      dimActorNumber = 1
      syncActorNumber = 5
      ignoreCase = true
      initQryBulkSize = 100
      initQryInterval = 1000
      startH2 = true
      tcpPort = ${HOST_LOCAL.PORTS.DC_TCP}
      database {
        username = sa
        connectionPool {
          initialsize = 10
          maxtotal = 10
          maxwaitmillis = 60000
        }
      }
      # For HA setup:
      clusteredCM = ${?HA.cmHA}
    }
  }
  //--------------------------------------------------------------------------------
  bulkOperation {
    akka = ${akka}{
      remote {
        netty.tcp {
          port = ${HOST_LOCAL.PORTS.AKKA_BULK_OP}
        }
      }
      remote.artery {
        canonical {
          port = ${HOST_LOCAL.PORTS.AKKA_BULK_OP}
        }
      }
    }
    slick-h2 {
      profile = "slick.jdbc.H2Profile$"
      db {
        connectionPool = disabled
        dataSourceClass = "slick.jdbc.DriverDataSource"
        properties = {
          driver = "org.h2.Driver"
          url = ${BULK_OP.CM_H2_DBURL}
          user = "sa"
        }
      }
    }
    slick-receiver {
      profile = "slick.jdbc.H2Profile$"
      db {
        connectionPool = disabled
        dataSourceClass = "slick.jdbc.DriverDataSource"
        properties = {
          driver = "org.h2.Driver"
          url = "jdbc:h2:tcp://localhost:"${HOST_LOCAL.PORTS.H2_RECEIVER_DB}"/mem:uocCenterPool"
          user = "sa"
        }
      }
    }
    presenter {
      protocol = ${HOST_PRESENTER.PROTOCOL}
      host = ${HOST_PRESENTER.HOST}
      port = ${HOST_PRESENTER.PORT}
    }
    ossm_cluster {
      otherBulkOpAdapters = ${?HA.bulkHA}
    }
  }
  //--------------------------------------------------------------------------------
  selfMonitor_threshold {
    akka = ${akka}{
      remote.artery {
        canonical {
          port = ${HOST_LOCAL.PORTS.AKKA_SELF_MONITOR_THRESHOLD}
        }
      }
    }
    thresholds = [
      {
        name = "jvm_heap_utilization",  #name+ leveal shall be unique in the list
        source = "jvm_mem"  #only support calculate from a sinlge kpi, for multi sources need take care the ts of them.
        formula = "used/max*100.0"
        type = >  # in [>,<,>=,<=]
        value = 1.0
        unit = "%"
        level = "Critical"
      }
    ]
  }
  //--------------------------------------------------------------------------------
  selfMonitor {
    akka = ${akka}{
      remote.artery {
        canonical {
          port = ${HOST_LOCAL.PORTS.AKKA_SELF_MONITOR}
        }
      }
    }
    collector {
      receive_db_monitor {
        interval = 60
      }
      SCRIPT_PATH = ${OSSM_HOME}/packages/self-monitor/script
      host_resource {
        interval = 60
        scripts {
          cpu = ${modules.selfMonitor.collector.SCRIPT_PATH}/cpu.sh
          memory = ${modules.selfMonitor.collector.SCRIPT_PATH}/memory.sh
        }
      }
      package_info {
        scripts {
          info = ${modules.selfMonitor.collector.SCRIPT_PATH}/ossmpackages.sh
        }
      }
      process_info {
        interval = 60
        scripts {
          info = ${modules.selfMonitor.collector.SCRIPT_PATH}/ossmshow.sh
        }
      }
    }
  }
}

